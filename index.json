[{"authors":["Abel Ang Beng Heng","Beng Heng Ang"],"categories":null,"content":"Upcoming 4th Year Integrative Sciences and Engineering Programme (ISEP) PhD Student supervised by Prof Ng See-Kiong.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d50ab9cf5dcffb8f83b8fb8f3f2845b1","permalink":"https://nlp-sig.github.io/author/abel-ang-beng-heng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/abel-ang-beng-heng/","section":"authors","summary":"Upcoming 4th Year Integrative Sciences and Engineering Programme (ISEP) PhD Student supervised by Prof Ng See-Kiong.","tags":null,"title":"Abel Ang Beng Heng","type":"authors"},{"authors":["Brian Formento"],"categories":null,"content":"Hi! I’m Brian Formento, A PhD candidate in the school of computer science at the National University of Singapore, doing research in the exciting field of deep learning! I’m a recipient of the SINGA scholarship and under the supervision of Professor See-Kiong Ng, Dr. Chen Zhenghua, and Dr. Chuan Sheng Foo. Before Singapore, I graduated with a MEng in electronic engineering from the University of Southampton. I was originally born in Turin Italy and moved to England at a young age, where I lived across the country, predominantly Reading, Bath, Portsmouth and Birmingham. I’m passionate about deep learning, machine learning, natural language processing, computer vision, and web development.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b179bcc3684b48be0d7ee538cc841452","permalink":"https://nlp-sig.github.io/author/brian-formento/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/brian-formento/","section":"authors","summary":"Hi! I’m Brian Formento, A PhD candidate in the school of computer science at the National University of Singapore, doing research in the exciting field of deep learning! I’m a recipient of the SINGA scholarship and under the supervision of Professor See-Kiong Ng, Dr.","tags":null,"title":"Brian Formento","type":"authors"},{"authors":["DENG Yang"],"categories":null,"content":"I am currently a postdoctoral research fellow in NExT++, School of Computing, National University of Singapore, working with Prof. Tat-Seng Chua and Prof. See-Kiong Ng. Before that, I received my Ph.D degree at The Chinese University of Hong Kong, supervised by Prof. Wai Lam.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bf0ca91b3ec5e15d79a0301deba0e238","permalink":"https://nlp-sig.github.io/author/deng-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/deng-yang/","section":"authors","summary":"I am currently a postdoctoral research fellow in NExT++, School of Computing, National University of Singapore, working with Prof. Tat-Seng Chua and Prof. See-Kiong Ng. Before that, I received my Ph.","tags":null,"title":"DENG Yang","type":"authors"},{"authors":["Dilruk Perera"],"categories":null,"content":"A Computer Science PhD graduate from the National University of Singapore, and a recipient of the Dean’s Graduate Research Excellence Award for the research achievements during the candidature. My expertise is in Machine Learning, Artificial Intelligence, Deep Learning, Information Retrieval and Data Analysis.\nIn addition to my academic pursuits, I have over 3 years of industry experience as a software engineer and research scientist, where I have led teams and worked with clients to deliver cutting-edge solutions.\nMy research and career interest is to develop intelligent solutions to effectively address pressing problems in the world using mathematical modeling and optimizations.\nCurrently, I am applying my research skills to make Smart Personalized Healthcare a reality and enhance the quality of people’s lives.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0485a039e3886ac61b9b56c5027df901","permalink":"https://nlp-sig.github.io/author/dilruk-perera/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dilruk-perera/","section":"authors","summary":"A Computer Science PhD graduate from the National University of Singapore, and a recipient of the Dean’s Graduate Research Excellence Award for the research achievements during the candidature. My expertise is in Machine Learning, Artificial Intelligence, Deep Learning, Information Retrieval and Data Analysis.","tags":null,"title":"Dilruk Perera","type":"authors"},{"authors":["Du Mingzhe","Mingzhe Du"],"categories":null,"content":"Du Mingzhe is a Research Assistant at the Institute of Data Science, National University of Singapore, working closely with Prof. See-Kiong Ng. He is also a first-year PhD student at the School of Computer Science and Engineering, Nanyang Technological University, supervised by Prof. Luu Anh Tuan. Prior to this, Mingzhe developed Tiktok search engine at ByteDance.\nIn recent times, Mingzhe has concentrated his research efforts on natural language processing (NLP) and its associated downstream tasks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c015834bdba2fb7be9d1ddb82fdb4199","permalink":"https://nlp-sig.github.io/author/du-mingzhe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/du-mingzhe/","section":"authors","summary":"Du Mingzhe is a Research Assistant at the Institute of Data Science, National University of Singapore, working closely with Prof. See-Kiong Ng. He is also a first-year PhD student at the School of Computer Science and Engineering, Nanyang Technological University, supervised by Prof.","tags":null,"title":"Du Mingzhe","type":"authors"},{"authors":["GE MENG"],"categories":null,"content":"I am currently a Research Fellow with [HLT Lab], Department of Electrical and Computer Engineering, National University of Singapore (NUS). I received Ph.D Degree in College of Intelligence and Computing in Tianjin University, supervised by Prof. Longbiao Wang and Prof. Jianwu Dang. My research topic is Speech Separation Based on Deep Learning in Open Complex Environment [Thesis] [Slide]. During my PhD period, I also worked as a research assistant in NTU and NUS in Singapore from 2019 to 2022, supervised by Prof. Eng Siong Chng and Prof. Haizhou Li. Prior to that, I received my Master’s Degree from Tianjin University under the supervision of Prof. Di Jin and Prof. Liang Yang in 2017, and my master’s research topic is Socical Network or Community Detection.\nMy research interest includes speech processing, speech separation and social network analysis. I have published more than 10 papers at the top international AI conferences such as IJCAI, ICASSP, INTERSPEECH.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ad2b32fb45c3b2a48eb1cfc2f6dd5994","permalink":"https://nlp-sig.github.io/author/ge-meng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ge-meng/","section":"authors","summary":"I am currently a Research Fellow with [HLT Lab], Department of Electrical and Computer Engineering, National University of Singapore (NUS). I received Ph.D Degree in College of Intelligence and Computing in Tianjin University, supervised by Prof.","tags":null,"title":"GE MENG","type":"authors"},{"authors":["He Kai"],"categories":null,"content":"Providing adequate medical care to everyone in the world remains a challenge, especially with many healthcare professionals already overworked. Training qualified doctors is a demanding task, and this contributes to the problem. One potential solution is the use of Artificial Intelligence (AI) to alleviate the workload of medical workers. AI has the potential to play a crucial role in the entire healthcare process, from predicting diseases, recommending medication, and providing prognosis.\nHe Kai and his colleagues are dedicated to researching medical AI, with the goal of effectively reducing the current pressure on medical workers. By integrating AI into healthcare, we can improve patient outcomes, enhance efficiency, and ultimately provide better medical care to people around the world.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"42e81651a4f4c0ddfa5ecde50cd956fd","permalink":"https://nlp-sig.github.io/author/he-kai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/he-kai/","section":"authors","summary":"Providing adequate medical care to everyone in the world remains a challenge, especially with many healthcare professionals already overworked. Training qualified doctors is a demanding task, and this contributes to the problem.","tags":null,"title":"He Kai","type":"authors"},{"authors":["Hu Wenyang"],"categories":null,"content":"Research Assistant supervised by A/Prof Bryan Low \u0026amp; Ng See-Kiong.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6859433fb87efe7307ce1d5f5b59a383","permalink":"https://nlp-sig.github.io/author/hu-wenyang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hu-wenyang/","section":"authors","summary":"Research Assistant supervised by A/Prof Bryan Low \u0026 Ng See-Kiong.","tags":null,"title":"Hu Wenyang","type":"authors"},{"authors":["Hu Zhiyuan","Zhiyuan Hu"],"categories":null,"content":"I am a PhD student in NUS IDS\u0026amp;SoC work with Bryan Hooi, See Kiong Ng and Luu Anh Tuan(NTU). Meanwhile, I am visisting UCL WI group cooperating with Aldo Lipani and Emine Yilmaz. Previously, I worked as a R\u0026amp;D engineer in natural language processing working with Prof. Zhang Richong, affiliated with BDBC and Computer Science College in Beihang University. My current research interests lie in LLLM applications on task-oriented dialogue system, reasoning and calibration capability of LLM. You can also find me in Google Scholar Personal Page https://zhiyuanhubj.github.io/\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1e1481d3df01d8a348f01f1833a55aa5","permalink":"https://nlp-sig.github.io/author/hu-zhiyuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hu-zhiyuan/","section":"authors","summary":"I am a PhD student in NUS IDS\u0026SoC work with Bryan Hooi, See Kiong Ng and Luu Anh Tuan(NTU). Meanwhile, I am visisting UCL WI group cooperating with Aldo Lipani and Emine Yilmaz.","tags":null,"title":"Hu Zhiyuan","type":"authors"},{"authors":["Huang Yixin"],"categories":null,"content":"I am currently a Research Intern at NUS IDS.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c0860439b2335640b6789bf95f199ab9","permalink":"https://nlp-sig.github.io/author/huang-yixin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/huang-yixin/","section":"authors","summary":"I am currently a Research Intern at NUS IDS.","tags":null,"title":"Huang Yixin","type":"authors"},{"authors":["Ji Bin"],"categories":null,"content":"I am a postdoctoral research fellow at the National University of Singapore (NUS), working with Prof. See-Kiong Ng. My research interests include Large Language Model, Inforation Retrieval, Information Extraction.\nMore on https://jibin5167.github.io/\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a7a9e792dc2ebc11266923f5d500aa70","permalink":"https://nlp-sig.github.io/author/ji-bin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ji-bin/","section":"authors","summary":"I am a postdoctoral research fellow at the National University of Singapore (NUS), working with Prof. See-Kiong Ng. My research interests include Large Language Model, Inforation Retrieval, Information Extraction.\nMore on https://jibin5167.","tags":null,"title":"Ji Bin","type":"authors"},{"authors":["Jinlan Fu"],"categories":null,"content":"Hi there, I am a postdoc at the National University of Singapore, working with Prof. See-Kiong Ng. I received my Ph.D. degree from the School of Computer Science, Fudan University (Sep. 2016 ~ Jul. 2021), supervised by Prof. Xuanjing Huang and Prof. Qi Zhang . From Dec. 2019 to Jun. 2022, I was lucky to work closely (remotely) with Dr. Pengfei Liu and Prof. Graham Neubig of the Language Technologies Institute (LTI) at Carnegie Mellon University.\nMy research focuses on the following perspectives for natural language processing:\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a8d92c724f63c537590c605a0a6e0cab","permalink":"https://nlp-sig.github.io/author/jinlan-fu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jinlan-fu/","section":"authors","summary":"Hi there, I am a postdoc at the National University of Singapore, working with Prof. See-Kiong Ng. I received my Ph.D. degree from the School of Computer Science, Fudan University (Sep.","tags":null,"title":"Jinlan Fu","type":"authors"},{"authors":["Mengling Feng"],"categories":null,"content":"Dr. Mengling Feng (https://www.mornin-feng.com/) is currently an Assistant Professor at National University of Singapore with School of Public Health, School of Medicine and School of Computing. He is also the Senior Assistant Director of National University Hospital championing the big data analytics efforts. Dr Feng is also an affiliated scientist with the Lab of Computational Physiology, Harvard-MIT Health Science Technology Division. His research is to develop machine learning algorithms to extract actionable knowledge from large amount of data.His research brings together concepts and tools across deep learning, optimization, signal processing, statistical causal inference and big data management. Dr. Feng’s work was recognized by both well-established journals, such as Science Translational Medicine, and top international conferences, such as KDD, AAAI and AMIA. In particular, he has been applying his AI expertise for both healthcare and financial applications. His team recently ranked number 2 in an international challenge on AI tools for medical image analysis. Dr Feng also co-founded an algo-trading company.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2fdd3b880b5cfcdb9b26a07c68c6fffa","permalink":"https://nlp-sig.github.io/author/mengling-feng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mengling-feng/","section":"authors","summary":"Dr. Mengling Feng (https://www.mornin-feng.com/) is currently an Assistant Professor at National University of Singapore with School of Public Health, School of Medicine and School of Computing. He is also the Senior Assistant Director of National University Hospital championing the big data analytics efforts.","tags":null,"title":"Mengling Feng","type":"authors"},{"authors":["See-Kiong Ng"],"categories":null,"content":"See-Kiong Ng (Ph.D., Carnegie Mellon University), a recipient of the Singapore National Computer Board’s overseas scholarship (1986), is currently Professor of Practice at the Department of Computer Science of the School of Computing, National University of Singapore (NUS), and Director, Translational Research for the university’s Institute of Data Science. Founded in May 2016, the Institute is the focal point at NUS for developing integrated data science capabilities and nurturing data scientists for Singapore’s Smart Nation initiative. It pushes the boundary of data science through transdisciplinary upstream research, and creates impact by translating the research outcomes into real-life applications by collaborating with partners from the industry and public agencies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b9ba11c27ea2ec52524c865a13d21a13","permalink":"https://nlp-sig.github.io/author/see-kiong-ng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/see-kiong-ng/","section":"authors","summary":"See-Kiong Ng (Ph.D., Carnegie Mellon University), a recipient of the Singapore National Computer Board’s overseas scholarship (1986), is currently Professor of Practice at the Department of Computer Science of the School of Computing, National University of Singapore (NUS), and Director, Translational Research for the university’s Institute of Data Science.","tags":null,"title":"See-Kiong Ng","type":"authors"},{"authors":["Thong T. Nguyen","Thong Nguyen"],"categories":null,"content":"I am a PhD student at National University of Singapore, advised by Asst. Prof. Anh-Tuan Luu and Prof. See-Kiong Ng. Currently, my research interests are Natural Language Processing, particularly Monolingual and Cross-Lingual Summarization, and Machine Learning, specifically Representation Learning and Optimization.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"87a32882419c0347e219a35a8c5e2f8d","permalink":"https://nlp-sig.github.io/author/thong-t.-nguyen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/thong-t.-nguyen/","section":"authors","summary":"I am a PhD student at National University of Singapore, advised by Asst. Prof. Anh-Tuan Luu and Prof. See-Kiong Ng. Currently, my research interests are Natural Language Processing, particularly Monolingual and Cross-Lingual Summarization, and Machine Learning, specifically Representation Learning and Optimization.","tags":null,"title":"Thong T. Nguyen","type":"authors"},{"authors":["Wu Zhaoxuan"],"categories":null,"content":"I am a Ph.D. student in Data Science at the National University of Singapore (NUS), supervised by Assoc. Prof. Bryan Kian Hsiang Low. I received my Bachelor of Science (Honors) in Data Science \u0026amp; Analytics and a minor in Computer Science from NUS in 2020. I am currently supported by the ISEP-IDS Scholarship jointly offered by the NUS Graduate School Integrative Sciences and Engineering Programme (ISEP) and the Institute of Data Science (IDS).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbd29101a0f44e7aa9aaa88db8d71c48","permalink":"https://nlp-sig.github.io/author/wu-zhaoxuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wu-zhaoxuan/","section":"authors","summary":"I am a Ph.D. student in Data Science at the National University of Singapore (NUS), supervised by Assoc. Prof. Bryan Kian Hsiang Low. I received my Bachelor of Science (Honors) in Data Science \u0026 Analytics and a minor in Computer Science from NUS in 2020.","tags":null,"title":"Wu Zhaoxuan","type":"authors"},{"authors":["Xiang Lan"],"categories":null,"content":"I am a Ph.D. student at National University of Singapore (NUS), under the supervision of Prof. Feng Mengling. My research focuses on designing and applying machine learning algorithms that help to solve real-world healthcare problems. Before Ph.D., I graduated from NUS with a master’s degree in electrical \u0026amp; computer engineering and did my research intern at IBM Research. Before Master, I graduated from the University of Electronic Science and Technology of China (UESTC) with a bachelor’s degree in electronic information science \u0026amp; technology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6ad84bea712a2e9720beec5a5396b382","permalink":"https://nlp-sig.github.io/author/xiang-lan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiang-lan/","section":"authors","summary":"I am a Ph.D. student at National University of Singapore (NUS), under the supervision of Prof. Feng Mengling. My research focuses on designing and applying machine learning algorithms that help to solve real-world healthcare problems.","tags":null,"title":"Xiang Lan","type":"authors"},{"authors":["Yucheng Ruan"],"categories":null,"content":"3rd year PhD student under Prof. Mornin\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"39dae733631a819c372eea4107033432","permalink":"https://nlp-sig.github.io/author/yucheng-ruan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yucheng-ruan/","section":"authors","summary":"3rd year PhD student under Prof. Mornin","tags":null,"title":"Yucheng Ruan","type":"authors"},{"authors":["Zhong Muhan"],"categories":null,"content":"Zhong Muhan is currently a Research Assistant, supervised by Prof Ng See-Kiong \u0026amp; Fu Jinlan\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2cf8a02de40fc245dd1c6e9399cab359","permalink":"https://nlp-sig.github.io/author/zhong-muhan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhong-muhan/","section":"authors","summary":"Zhong Muhan is currently a Research Assistant, supervised by Prof Ng See-Kiong \u0026 Fu Jinlan","tags":null,"title":"Zhong Muhan","type":"authors"},{"authors":[],"categories":null,"content":"Talk Title: Towards robustness of AI models for Natural Language Processing\nAbstract: AI models, especially Large Language Models have revolutionized the field of Natural Language Processing by achieving state-of-the-art results in a wide range of tasks. However, their widespread adoption has also raised concerns about their vulnerabilities to various adversarial attacks. In this talk, I will introduce our recent efforts in exploring the robustness of current AI architectures for NLP and how to defend them against adversarial attacks.\nSpeaker: Anh Tuan Luu (https://tuanluu.github.io/), the Assistant Professor at School of Computer Science and Engineering, NTU. Before that, he was a postdoctoral fellow at MIT CSAIL and a member of the NLP group under the supervision of Prof. Regina Barzilay from 2018 to 2020. He obtained Ph.D. degree in computer engineering from NTU in 2016. From 2016 to 2018, he was a research scientist at Institute for Infocomm Research, Singapore in the Natural Language Processing group of Dr. Jian Su. His research interests lie in the intersection of Artificial Intelligence and Natural Language Processing with the focus on applications of pretrained language models, semantics, question \u0026amp; answering, information extraction and knowledge construction, robustness \u0026amp; trustworthy AI, and recommendation systems.\nTalk title: Do LLMs Excel in Every Aspect? Overcoming Limitations through Scalable Learning\nAbstract: The advent of large-scale language models like ChatGPT and GPT-4 has revolutionized our understanding and capabilities. However, the challenges of hallucination, scalability, and adaptability remain. This talk presents three of our research efforts that address these challenges. First, I will discuss a framework to distill commonsense knowledge from large language models, enabling commonsense reasoning at scale. Second, I will introduce a general-purpose commonsense verification language model that surpasses the performance of its larger counterparts, thereby offering a more resource-efficient yet effective alternative. Finally, I will explore the concept of in-context adaptation learning for smaller-scale language models, demonstrating their ability to generalize across different domains on some typical NLP tasks. In the end, I will discuss remaining challenges and future research directions.\nSpeaker: Wenya Wang (https://personal.ntu.edu.sg/wangwy/). She is an Assistant Professor with the school of Computer Science and Engineering at NTU. Prior to joining NTU, she worked with Noah Smith and Hanna Hajishirzi as a Postdoc in Paul G. Allen School of Computer Science and Engineering at the University of Washington. She completed my PhD under the supervision of Sinno Jialin Pan at NTU. Her main research interests lie in reasoning for Natural Language Processing (or multimodal learning), including logic reasoning, commonsense reasoning, knowledge integration etc.\nEvent Details:\nDate: Friday, 3rd November Time: 10:00 AM Venue: Conference Room, First Floor, Innovation 4.0, NUS (3 Research Link, Singapore 117602) We are eagerly anticipating this enriching seminar and hope to see you there for a morning of discussions and networking. Following the seminar, we will also be hosting an NLP SIG workshop(in the morning of 17th Nov, Friday), showcasing around 10 papers recently accepted by prestigious conferences. Please stay tuned for this upcoming event!\n","date":1699005600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699005600,"objectID":"8a67ff6b9b4362a60e2d5ebf6c4071e1","permalink":"https://nlp-sig.github.io/event/0311_reading/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/0311_reading/","section":"event","summary":"IDS-CS Seminar on Large Language Models: Friday 03/11, 10am at I4","tags":[],"title":"IDS NLP SIG Research Seminar","type":"event"},{"authors":[],"categories":null,"content":"We are delighted to share that our NLP SIG seminar on Large Language Models (LLMs), held on the 3rd of November, was a notable gathering, underscored by the presence of two luminaries in the field.\nDistinguished Speakers: Anh Tuan Luu: A venerated authority with a storied career from postdoctoral research at MIT to impactful roles in NLP at NTU and I2R, Luu is at the forefront of fortifying the robustness of AI systems. Wenya Wang: A trailblazing researcher whose postdoctoral work at the University of Washington has given rise to innovative methodologies for enhancing the scalability and reasoning capabilities of AI. Highlights from the Talks: Anh Tuan Luu offered a deep dive into the resilience of AI models in NLP, spotlighting pioneering work in protecting AI infrastructures against adversarial encroachments to bolster their reliability and trustworthiness.\nWenya Wang confronted the inherent challenges of hallucinations, scalability, and adaptability in modern LLMs like GPT-4. Her avant-garde research introduces more sophisticated, capable language models primed for commonsense reasoning and swift adaptation across a spectrum of NLP tasks.\nInteractive Q\u0026amp;A Session: The talks were followed by a vibrant interactive Q\u0026amp;A session, allowing attendees to engage directly with the speakers, fostering a rich exchange of ideas and clarifying complex topics in real-time.\n","date":1699005600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699005600,"objectID":"2d57fbd50c0b0492bd4977ff43feb69f","permalink":"https://nlp-sig.github.io/event/0311_reading_recap/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/0311_reading_recap/","section":"event","summary":"IDS-CS Seminar on Large Language Models: Friday 03/11, 10am at I4","tags":[],"title":"Seminar Recap: NLP SIG Monthly Research Event – Large Language Models Focus","type":"event"},{"authors":["Beng Heng Ang","Sujatha Das Gollapalli","See-Kiong Ng"],"categories":null,"content":"Link: https://aclanthology.org/2023.eacl-main.12/\n","date":1696204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696204800,"objectID":"c6c3b97278a73a3518a3f05313f35f61","permalink":"https://nlp-sig.github.io/publication/socratic-question-generation-a-novel-dataset-models-and-evaluation/","publishdate":"2023-10-02T00:00:00Z","relpermalink":"/publication/socratic-question-generation-a-novel-dataset-models-and-evaluation/","section":"publication","summary":"Socratic questioning is a form of reflective inquiry often employed in education to encourage critical thinking in students, and to elicit awareness of beliefs and perspectives in a subject during therapeutic counseling. Specific types of Socratic questions are employed for enabling reasoning and alternate views against the context of individual personal opinions on a topic. Socratic contexts are different from traditional question generation contexts where “answer-seeking” questions are generated against a given formal passage on a topic, narrative stories or conversations. We present SocratiQ, the first large dataset of 110K (question, context) pairs for enabling studies on Socratic Question Generation (SoQG). We provide an in-depth study on the various types of Socratic questions and present models for generating Socratic questions against a given context through prompt tuning. Our automated and human evaluation results demonstrate that our SoQG models can produce realistic, type-sensitive, human-like Socratic questions enabling potential applications in counseling and coaching.","tags":null,"title":"Socratic Question Generation: A Novel Dataset, Models, and Evaluation","type":"publication"},{"authors":[],"categories":null,"content":"We are witnessing the widespread adoption of deep learning techniques as avant-garde solutions to different computational problems in recent years. In data integration, the use of deep learning techniques has helped establish several state-of-the-art results in long standing problems, including information extraction, entity matching, data cleaning, and table understanding. In this talk, I will reflect on the strengths of deep learning and how that has helped move forward the needle in data integration. I will also discuss a few challenges associated with solutions based on deep learning techniques and describe some opportunities for future work. Recently, Large Language Models (LLMs) have emerged as a powerful tool for accessing parametric knowledge, but the potential of tapping into the vast expanse of external or private data remains largely unexplored. This talk presents an open-source question-answering system for seamlessly integrating model parameters with knowledge from external data sources to enhance its predictive capabilities. Our larger vision transcends question answering. We envision a personal insight assistant, adept at sifting through your past data to offer invaluable insights to help make informed decisions and plan with foresight.\nWang-Chiew is a research scientist at Meta AI. Before she was the Head of Research at Megagon Labs, where she led the research efforts on building advanced technologies to enhance search by experience. Prior to joining Megagon Labs, she was a Professor of Computer Science at the University of California, Santa Cruz. She also spent two years at IBM Research - Almaden. She received her B.Sc. (First Class) in Computer Science from the National University of Singapore and her Ph.D. in Computer Science from the University of Pennsylvania. Her research interests include data integration and exchange, data provenance, and natural language processing. She is the recipient of an NSF CAREER award, a Google Faculty Award, and an IBM Faculty Award. She co-authored best papers, she is a co-recipient of the 2014 ACM PODS Alberto O. Mendelzon Test-of-Time Award, the 2018 ICDT Test-of-Time Award, and the 2020 Alonzo Church Award. She received the 2019 VLDB Women in Database Research Award. She was on the VLDB Board of Trustees (2014-2019) and she is a Fellow of the ACM.\n","date":1695722400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695722400,"objectID":"ead9675e15f15b1952de894bb8f4d8d1","permalink":"https://nlp-sig.github.io/event/from_deep_data_integration_to_using_llms_to_query_unstructured_and_structured_data/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/from_deep_data_integration_to_using_llms_to_query_unstructured_and_structured_data/","section":"event","summary":"IDS-CS Seminar on entity linking and LLMs: Tuesday 10am at COM3 MPH","tags":[],"title":"From Deep Data Integration to Using LLMs to Query Unstructured and Structured Data","type":"event"},{"authors":[],"categories":null,"content":"Institute of Data Science played host to the much-anticipated Natural Language Processing (NLP) Special Interest Group (SIG) Kickoff Event on August 8, 2023.\nThe Kickoff Event was a golden opportunity for NLP enthusiasts to gather and explore the latest trends in NLP research, connect, and spark collaborations. The NLP SIG at IDS was initiated with the vision of bringing together a community interested in the intersections of language with machine learning, AI, and data science. This Kickoff Event marked the formal commencement of this groundbreaking initiative.\nStay tuned for more updates on future events and networking opportunities from NLP SIG at IDS. Join the league in shaping the future of natural language processing!\n","date":1692788400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692788400,"objectID":"1f93d088966641c26deba0313626a6eb","permalink":"https://nlp-sig.github.io/event/kickoff/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/kickoff/","section":"event","summary":"NLP SIG Kickoff Event @ IDS","tags":[],"title":"NLP SIG Kickoff Event","type":"event"},{"authors":null,"categories":null,"content":"Institute of Data Science played host to the much-anticipated Natural Language Processing (NLP) Special Interest Group (SIG) Kickoff Event on August 8, 2023.\nThe Kickoff Event was a golden opportunity for NLP enthusiasts to gather and explore the latest trends in NLP research, connect, and spark collaborations.\nThe NLP SIG at IDS was initiated with the vision of bringing together a community interested in the intersections of language with machine learning, AI, and data science. This Kickoff Event marked the formal commencement of this groundbreaking initiative.\nStay tuned for more updates on future events and networking opportunities from NLP SIG at IDS. Join the league in shaping the future of natural language processing!\n","date":1691452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691452800,"objectID":"57d18dab22b013100a64f23539bfc5e3","permalink":"https://nlp-sig.github.io/post/sig-kickoff/","publishdate":"2023-08-08T00:00:00Z","relpermalink":"/post/sig-kickoff/","section":"post","summary":"Institute of Data Science played host to the much-anticipated Natural Language Processing (NLP) Special Interest Group (SIG) Kickoff Event on August 8, 2023.\n","tags":null,"title":"NLP SIG Kickoff Event","type":"post"},{"authors":["Junjie Li","Meng Ge","Zexu pan","Rui Cao","Longbiao Wang","Jianwu Dang","Shiliang Zhang"],"categories":null,"content":"Link: https://arxiv.org/abs/2306.02625\n","date":1685923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685923200,"objectID":"2de661e60bf045d3bd7e546fe9a7fb1f","permalink":"https://nlp-sig.github.io/publication/rethinking-the-visual-cues-in-audio-visual-speaker-extraction./","publishdate":"2023-06-05T00:00:00Z","relpermalink":"/publication/rethinking-the-visual-cues-in-audio-visual-speaker-extraction./","section":"publication","summary":"The Audio-Visual Speaker Extraction (AVSE) algorithm employs parallel video recording to leverage two visual cues, namely speaker identity and synchronization, to enhance performance compared to audio-only algorithms. However, the visual front-end in AVSE is often derived from a pre-trained model or end-to-end trained, making it unclear which visual cue contributes more to the speaker extraction performance. This raises the question of how to better utilize visual cues. To address this issue, we propose two training strategies that decouple the learning of the two visual cues. Our experimental results demonstrate that both visual cues are useful, with the synchronization cue having a higher impact. We introduce a more explainable model, the Decoupled Audio-Visual Speaker Extraction (DAVSE) model, which leverages both visual cues.","tags":null,"title":"Rethinking the Visual Cues in Audio-Visual Speaker Extraction.","type":"publication"},{"authors":["Zhiyuan Hu","Yue Feng","Anh Tuan Luu","Bryan Hooi","Aldo Lipani"],"categories":null,"content":"Link: https://arxiv.org/abs/2306.09821\n","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"911bea6d9a2f1c864e358aa1731b33de","permalink":"https://nlp-sig.github.io/publication/demaformer-damped-exponential-moving-average-transformer-with-energy-based-modeling-for-temporal-language-grounding./","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/demaformer-damped-exponential-moving-average-transformer-with-energy-based-modeling-for-temporal-language-grounding./","section":"publication","summary":"Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.","tags":null,"title":"DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding","type":"publication"},{"authors":["Zhiyuan Hu","Yue Feng","Luu_Anh_Tuan","Bryan Hooi","Aldo Lipani"],"categories":null,"content":"Link: https://arxiv.org/abs/2306.09821\n","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"654ac96dcdb41a0f6692a9406d7e675d","permalink":"https://nlp-sig.github.io/publication/unlocking-the-potential-of-user-feedbackleveraging-large-language-model-as-user-simulators-to-enhance-dialogue-system/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/unlocking-the-potential-of-user-feedbackleveraging-large-language-model-as-user-simulators-to-enhance-dialogue-system/","section":"publication","summary":"Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.","tags":["CIKM"],"title":"Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System","type":"publication"},{"authors":["Brian Formento","Chuan Sheng Foo","Luu_Anh_Tuan","See-Kiong Ng"],"categories":null,"content":"Link: https://aclanthology.org/2023.findings-eacl.1/\n","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"02e35865e34409410249caf038b05865","permalink":"https://nlp-sig.github.io/publication/using-punctuation-as-an-adversarial-attack-on-deep-learning-based-nlp-systems-an-empirical-study/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/publication/using-punctuation-as-an-adversarial-attack-on-deep-learning-based-nlp-systems-an-empirical-study/","section":"publication","summary":"This work empirically investigates punctuation insertions as adversarial attacks on NLP systems. Data from experiments on three tasks, five datasets, and six models with four attacks show that punctuation insertions, when limited to a few symbols (apostrophes and hyphens), are a superior attack vector compared to character insertions due to 1) a lower after-attack accuracy (Aaft-atk) than alphabetical character insertions; 2) higher semantic similarity between the resulting and original texts; and 3) a resulting text that is easier and faster to read as assessed with the Test of Word Reading Efficiency (TOWRE)). The tests also indicate that 4) grammar checking does not mitigate punctuation insertions and 5) punctuation insertions outperform word-level attacks in settings with a limited number of word synonyms and queries to the victim’s model. Our findings indicate that inserting a few punctuation types that result in easy-to-read samples is a general attack mechanism. In light of this threat, we assess the impact of punctuation insertions, potential mitigations, the mitigation’s tradeoffs, punctuation insertion’s worst-case scenarios and summarize our findings in a qualitative casual map, so that developers can design safer, more secure systems.","tags":["EACL"],"title":"Using Punctuation as an Adversarial Attack on Deep Learning-Based NLP Systems:An Empirical Study","type":"publication"},{"authors":["Fanghua Ye","Zhiyuan Hu","Emine Yilmaz"],"categories":null,"content":"Link: https://arxiv.org/abs/2305.12594\n","date":1684627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684627200,"objectID":"bcb481f1b640e2068c524e57fb5c4e97","permalink":"https://nlp-sig.github.io/publication/modeling-user-satisfaction-dynamics-in-dialogue-via-hawkes-process/","publishdate":"2023-05-21T00:00:00Z","relpermalink":"/publication/modeling-user-satisfaction-dynamics-in-dialogue-via-hawkes-process/","section":"publication","summary":"Dialogue systems have received increasing attention while automatically evaluating their performance remains challenging. User satisfaction estimation (USE) has been proposed as an alternative. It assumes that the performance of a dialogue system can be measured by user satisfaction and uses an estimator to simulate users. The effectiveness of USE depends heavily on the estimator. Existing estimators independently predict user satisfaction at each turn and ignore satisfaction dynamics across turns within a dialogue. In order to fully simulate users, it is crucial to take satisfaction dynamics into account. To fill this gap, we propose a new estimator ASAP (sAtisfaction eStimation via HAwkes Process) that treats user satisfaction across turns as an event sequence and employs a Hawkes process to effectively model the dynamics in this sequence. Experimental results on four benchmark dialogue datasets demonstrate that ASAP can substantially outperform state-of-the-art baseline estimators.","tags":null,"title":"Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process","type":"publication"},{"authors":["Yang Deng","Wenxuan Zhang","Yifei Yuan","Wai Lam"],"categories":null,"content":"Link: https://arxiv.org/abs/2305.10172\n","date":1684281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684281600,"objectID":"eacaa847a77184a40d0218edbb9d909e","permalink":"https://nlp-sig.github.io/publication/knowledge-enhanced-mixed-initiative-dialogue-system-for-emotional-support-conversations/","publishdate":"2023-05-17T00:00:00Z","relpermalink":"/publication/knowledge-enhanced-mixed-initiative-dialogue-system-for-emotional-support-conversations/","section":"publication","summary":"Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimental results on two ESC datasets show the superiority of KEMI in both content-preserving evaluation and mixed initiative related analyses.","tags":null,"title":"Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations","type":"publication"},{"authors":["Thong Nguyen","Xiaobao Wu","Xinshuai Dong","Cong-Duy T. Nguyen","Zhen Hai","Lidong Bing","Luu_Anh_Tuan"],"categories":null,"content":"Link: https://arxiv.org/abs/2305.12678\n","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"1227792b76a886335f5be9d80e2529b5","permalink":"https://nlp-sig.github.io/publication/gradient-boosted-decision-tree-for-listwise-context-model-in-multimodal-review-helpfulness-prediction/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/gradient-boosted-decision-tree-for-listwise-context-model-in-multimodal-review-helpfulness-prediction/","section":"publication","summary":"Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predictor and pairwise loss as the training objective. However, FCNNs have been shown to perform inefficient splitting for review features, making the model difficult to clearly differentiate helpful from unhelpful reviews. Furthermore, pairwise objective, which works on review pairs, may not completely capture the MRHP goal to produce the ranking for the entire review list, and possibly induces low generalization during testing. To address these issues, we propose a listwise attention network that clearly captures the MRHP ranking context and a listwise optimization objective that enhances model generalization. We further propose gradient-boosted decision tree as the score predictor to efficaciously partition product reviews' representations. Extensive experiments demonstrate that our method achieves state-of-the-art results and polished generalization performance on two large-scale MRHP benchmark datasets.","tags":["ACL"],"title":"Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction","type":"publication"},{"authors":["Qinghua Liu","Meng Ge","Zhizheng Wu","Haizhou Li"],"categories":null,"content":"Link: https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html\n","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"256f8860986c09fac0068fdc891b6298","permalink":"https://nlp-sig.github.io/publication/piave-a-pose-invariant-audio-visual-speaker-extraction-network/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/piave-a-pose-invariant-audio-visual-speaker-extraction-network/","section":"publication","summary":"It is common in everyday spoken communication that we look at the turning head of a talker to listen to his/her voice. Humans see the talker to listen better, so do machines. However, previous studies on audio-visual speaker extraction have not effectively handled the varying talking face. This paper studies how to take full advantage of the varying talking face. We propose a Pose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) that incorporates an additional pose-invariant view to improve audio-visual speaker extraction. Specifically, we generate the pose-invariant view from each original pose orientation, which enables the model to receive a consistent frontal view of the talker regardless of his/her head pose, therefore, forming a multi-view visual input for the speaker. Experiments on the multi-view MEAD and in-the-wild LRS3 dataset demonstrate that PIAVE outperforms the state-of-the-art and is more robust to pose variations.","tags":null,"title":"PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network","type":"publication"},{"authors":["Yang Deng","Lizi Liao","Liang Chen","Hongru Wang","Wenqiang Lei","Tat-Seng Chua"],"categories":null,"content":"Link: https://arxiv.org/abs/2305.13626\n","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"70709c98fb61e2247f4b953cf93a4b10","permalink":"https://nlp-sig.github.io/publication/prompting-and-evaluating-large-language-models-for-proactive-dialogues-clarification-target-guided-and-non-collaboration./","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/prompting-and-evaluating-large-language-models-for-proactive-dialogues-clarification-target-guided-and-non-collaboration./","section":"publication","summary":"Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.","tags":null,"title":"Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration","type":"publication"},{"authors":["Xiaoqiang Lin*","Zhaoxuan Wu*","Zhongxiang Dai","Wenyang Hu","Yao Shu","See-Kiong Ng","Patrick Jaillet","Bryan Kian Hsiang Low"],"categories":null,"content":"Link: https://arxiv.org/abs/2310.02905\n","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"3c713fa220bb044df222f409ca81c8be","permalink":"https://nlp-sig.github.io/publication/use-your-instinct-instruction-optimization-using-neural-bandits-coupled-with-transformers./","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/use-your-instinct-instruction-optimization-using-neural-bandits-coupled-with-transformers./","section":"publication","summary":"Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers} (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that our INSTINCT consistently outperforms the existing methods in different tasks, such as in various instruction induction tasks and the task of improving the zero-shot chain-of-thought instruction.","tags":null,"title":"Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers","type":"publication"},{"authors":["Yang Deng","Wenxuan Zhang","Qian Yu","Wai Lam"],"categories":null,"content":"Link: https://arxiv.org/abs/2302.08092\n","date":1676505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676505600,"objectID":"956a7c69093da1fdd6bd1c890a067fa0","permalink":"https://nlp-sig.github.io/publication/product-question-answering-in-e-commerce-a-survey/","publishdate":"2023-02-16T00:00:00Z","relpermalink":"/publication/product-question-answering-in-e-commerce-a-survey/","section":"publication","summary":"Product question answering (PQA), aiming to automatically provide instant responses to customer's questions in E-Commerce platforms, has drawn increasing attention in recent years. Compared with typical QA problems, PQA exhibits unique challenges such as the subjectivity and reliability of user-generated contents in E-commerce platforms. Therefore, various problem settings and novel methods have been proposed to capture these special characteristics. In this paper, we aim to systematically review existing research efforts on PQA. Specifically, we categorize PQA studies into four problem settings in terms of the form of provided answers. We analyze the pros and cons, as well as present existing datasets and evaluation protocols for each setting. We further summarize the most significant challenges that characterize PQA from general QA applications and discuss their corresponding solutions. Finally, we conclude this paper by providing the prospect on several future directions.","tags":null,"title":"Product Question Answering in E-Commerce: A Survey","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://nlp-sig.github.io/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://nlp-sig.github.io/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"}]