---
title: NLP SIG Showcase – Nov 2024 Edition

authors:
- See-Kiong Ng

location: Seminar Room, Innovation 4.0, NUS (3 Research Link, Singapore 117602)


summary: 'NLP SIG Showcase – Nov 2024 Edition'
abstract: 'We are pleased to announce that NLP SIG Showcase – Nov 2024 Edition will be held on Monday, 04 November 2024.'

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2024-11-04T08:30:00Z'
date_end: '2024-11-04T15:00:00Z'
all_day: false

# # Schedule page publish date (NOT talk date).
publishDate: '2024-11-04T00:00:00Z'

authors: []
tags: []

# Is this a featured talk? (true/false)
featured: true

image:
  focal_point: Right

url_code: ''
url_pdf: ''
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides:

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
---

We am pleased to inform you about the upcoming IDS NLP SIG Workshop, where we will be showcasing cutting-edge papers published by the IDS students and researchers at top-tier conferences recently. This event will provide a unique opportunity for attendees to delve into the latest advancements in the field of NLP & ML research and interact with our students and researchers to find out more about the research activities at IDS.
 
In this workshop, we will feature two industry speakers (who are IDS and SOC alumni) from TikTok and Salesforce, presenting the latest research developments in industry.  The workshop will also showcase a wide range of recent research works done on the latest topics in LLMs and ML by poster presentation, where you can interact with the students and researchers about their work on a one-on-one basis.

### Event Details:
- Date: Monday, 4th November
- Time: 9:20 AM – 12:30 PM
- Venue: First Floor, Innovation 4.0, NUS (3 Research Link, Singapore 117602), near COM3 and COM4
 
### Schedule:
- 09:25 – 09:30: Opening Remark
- 09:30 – 10:00: Efficient Long Video Generation with Story Telling Capabilities, Daquan Zhou, TikTok
- 10:00 – 10: 30: Advancing Time Series Forecasting: Unified Transformers and Foundation Model with Mixture-of-Experts, Juncheng Liu, Salesforce
- 10:30 – 10:45: Teabreak
- 10:45 – 12:15: Poster Presentation

### Keynote Talks:
 
#### Efficient Long Video Generation with Story Telling Capabilities
Speaker: Daquan Zhou, TikTok
Bio: Daquan Zhou graduated from the National University of Singapore and joined ByteDance as a research scientist in 2022. He is the recipient of the Singapore Data Science Consortium (SDSC) PhD Dissertation Award in 2021. Previously, he contributed to the development of Singapore's first commercial artificial satellite (2016-2018). His paper, "Coordinate Attention for Efficient Mobile Network Design," is currently ranked fifth on the CVPR 2021 Most Influential List. His work on robustness, "Fully Attentional Network," was used as the foundation for the winning solution in five segmentation tracks of the 2022 Visual Robustness Challenge and was integrated as a base model into Nvidia's Developer TAO Toolkit.
Abstract: Video generation has been a hot research area in recent years. However, generating realistic, continuous, and long videos remains a challenging problem in the field. This talk explores how to design an efficient video generation architecture with temporal continuity and the ability to express a complete storyline from three perspectives: dataset generation, video generation model algorithm design, and computational overhead.
 
#### Advancing Time Series Forecasting: Unified Transformers and Foundation Model with Mixture-of-Experts
Speaker: Juncheng Liu, Salesforce
Bio: Juncheng is currently a Research Scientist at Salesforce AI Research. He obtained PhD in Computer Science at School of Computing (SoC), National University of Singapore (NUS), advised by Prof. Xiaokui Xiao. His research interests are time series foundation models, graph learning, and related applications. He is the recipient of 2022 ACM SIGMOD Research Highlight Award and Best Research Paper Award in VLDB 2021.
Abstract: This talk will focus on our two recent works on time series forecasting at Salesforce. The first, UniTST, introduces a unified Transformer model that captures both variate and temporal dependencies, offering strong performance with a simple architecture. The second, Moirai-MoE, builds on our previous time series foundation model Moirai. It is the first mixture-of-experts model for time series, achieving token-level specialization and outperforming existing models with fewer parameters.


### Poster Presentations:
 
- Encoding and Controlling Global Semantics for Long-form Video Question Answering. Thong Nguyen, et al. EMNLP 2024
- Aligning Translation-Specific Understanding to General Understanding in Large Language Models. Yichong Huang, et al. EMNLP 2024
- MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration. Lin Xu, et al. EMNLP 2024
- One-Shot Sequential Federated Learning for Non-llD Data by Enhancing Local Model Diversity. Naibo Wang, et al. ACM MM 2024.
- Towards Effective Federated Graph Anomaly Detection via Self-boosted Knowledge Distillation. Jinyu Cai, et al. ACM MM 2024.
- Learning the Unlearned: Mitigating Feature Suppression in Contrastive Learning. Xiang Lan, et al. ECCV 2024.
- Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning. Thong Nguyen, et al. ECCV 2024
- Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models. Zhiyuan Hu, et al. NeurIPS 2024.
- Localized zeroth-order prompt optimization. Wenyang Hu, et al. NeurIPS 2024.
-An I/O efficient scheme for heterogeneous and distributed LLM operation. Yiqi Zhang, et al. NeurIPS 2024.
- Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration. Yichong Huang, et al. NeurIPS 2024.
- Mercury: A Code Efficiency Benchmark for Code Large Language Models. Mingzhe Du, et al. NeurIPS 2024.
- Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data. Zhaomin Wu, et al. NeurIPS 2024.
