<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>IDS NLP-SIG</title><link>https://nlp-sig.github.io/</link><atom:link href="https://nlp-sig.github.io/index.xml" rel="self" type="application/rss+xml"/><description>IDS NLP-SIG</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://nlp-sig.github.io/media/icon_hu16c578916c2a5a45536e9b6171a7f5af_32385_512x512_fill_lanczos_center_3.png</url><title>IDS NLP-SIG</title><link>https://nlp-sig.github.io/</link></image><item><title>IDS NLP SIG Research Seminar</title><link>https://nlp-sig.github.io/event/0311_reading/</link><pubDate>Fri, 03 Nov 2023 10:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/event/0311_reading/</guid><description>&lt;p>&lt;strong>Talk Title:&lt;/strong> Towards robustness of AI models for Natural Language Processing&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> AI models, especially Large Language Models have revolutionized the field of Natural Language Processing by achieving state-of-the-art results in a wide range of tasks. However, their widespread adoption has also raised concerns about their vulnerabilities to various adversarial attacks. In this talk, I will introduce our recent efforts in exploring the robustness of current AI architectures for NLP and how to defend them against adversarial attacks.&lt;/p>
&lt;p>&lt;strong>Speaker:&lt;/strong> Anh Tuan Luu (&lt;a href="https://tuanluu.github.io/%29" target="_blank" rel="noopener">https://tuanluu.github.io/)&lt;/a>, the Assistant Professor at School of Computer Science and Engineering, NTU. Before that, he was a postdoctoral fellow at MIT CSAIL and a member of the NLP group under the supervision of Prof. Regina Barzilay from 2018 to 2020. He obtained Ph.D. degree in computer engineering from NTU in 2016. From 2016 to 2018, he was a research scientist at Institute for Infocomm Research, Singapore in the Natural Language Processing group of Dr. Jian Su. His research interests lie in the intersection of Artificial Intelligence and Natural Language Processing with the focus on applications of pretrained language models, semantics, question &amp;amp; answering, information extraction and knowledge construction, robustness &amp;amp; trustworthy AI, and recommendation systems.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Talk title:&lt;/strong> Do LLMs Excel in Every Aspect? Overcoming Limitations through Scalable Learning&lt;/p>
&lt;p>&lt;strong>Abstract:&lt;/strong> The advent of large-scale language models like ChatGPT and GPT-4 has revolutionized our understanding and capabilities. However, the challenges of hallucination, scalability, and adaptability remain. This talk presents three of our research efforts that address these challenges. First, I will discuss a framework to distill commonsense knowledge from large language models, enabling commonsense reasoning at scale. Second, I will introduce a general-purpose commonsense verification language model that surpasses the performance of its larger counterparts, thereby offering a more resource-efficient yet effective alternative. Finally, I will explore the concept of in-context adaptation learning for smaller-scale language models, demonstrating their ability to generalize across different domains on some typical NLP tasks. In the end, I will discuss remaining challenges and future research directions.&lt;/p>
&lt;p>&lt;strong>Speaker:&lt;/strong> Wenya Wang (&lt;a href="https://personal.ntu.edu.sg/wangwy/%29" target="_blank" rel="noopener">https://personal.ntu.edu.sg/wangwy/)&lt;/a>. She is an Assistant Professor with the school of Computer Science and Engineering at NTU. Prior to joining NTU, she worked with Noah Smith and Hanna Hajishirzi as a Postdoc in Paul G. Allen School of Computer Science and Engineering at the University of Washington. She completed my PhD under the supervision of Sinno Jialin Pan at NTU. Her main research interests lie in reasoning for Natural Language Processing (or multimodal learning), including logic reasoning, commonsense reasoning, knowledge integration etc.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Event Details:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Date:&lt;/strong> Friday, 3rd November&lt;/li>
&lt;li>&lt;strong>Time:&lt;/strong> 10:00 AM&lt;/li>
&lt;li>&lt;strong>Venue:&lt;/strong> Conference Room, First Floor, Innovation 4.0, NUS (3 Research Link, Singapore 117602)&lt;/li>
&lt;/ul>
&lt;p>We are eagerly anticipating this enriching seminar and hope to see you there for a morning of discussions and networking. Following the seminar, we will also be hosting an NLP SIG workshop(in the morning of 17th Nov, Friday), showcasing around 10 papers recently accepted by prestigious conferences. Please stay tuned for this upcoming event!&lt;/p></description></item><item><title>Seminar Recap: NLP SIG Monthly Research Event â€“ Large Language Models Focus</title><link>https://nlp-sig.github.io/event/0311_reading_recap/</link><pubDate>Fri, 03 Nov 2023 10:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/event/0311_reading_recap/</guid><description>&lt;p>We are delighted to share that our NLP SIG seminar on Large Language Models (LLMs), held on the 3rd of November, was a notable gathering, underscored by the presence of two luminaries in the field.&lt;/p>
&lt;h2 id="distinguished-speakers">Distinguished Speakers:&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Anh Tuan Luu&lt;/strong>: A venerated authority with a storied career from postdoctoral research at MIT to impactful roles in NLP at NTU and I2R, Luu is at the forefront of fortifying the robustness of AI systems.&lt;/li>
&lt;li>&lt;strong>Wenya Wang&lt;/strong>: A trailblazing researcher whose postdoctoral work at the University of Washington has given rise to innovative methodologies for enhancing the scalability and reasoning capabilities of AI.&lt;/li>
&lt;/ul>
&lt;h2 id="highlights-from-the-talks">Highlights from the Talks:&lt;/h2>
&lt;p>Anh Tuan Luu offered a deep dive into the resilience of AI models in NLP, spotlighting pioneering work in protecting AI infrastructures against adversarial encroachments to bolster their reliability and trustworthiness.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="110301" srcset="
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_952a8bd18b07581d8b652f1774bf7fee.webp 400w,
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_a4dd9cd246dd4f9a1e10111d82773838.webp 760w,
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://nlp-sig.github.io/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_952a8bd18b07581d8b652f1774bf7fee.webp"
width="760"
height="422"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Wenya Wang confronted the inherent challenges of hallucinations, scalability, and adaptability in modern LLMs like GPT-4. Her avant-garde research introduces more sophisticated, capable language models primed for commonsense reasoning and swift adaptation across a spectrum of NLP tasks.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="110301" srcset="
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_952a8bd18b07581d8b652f1774bf7fee.webp 400w,
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_a4dd9cd246dd4f9a1e10111d82773838.webp 760w,
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://nlp-sig.github.io/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_952a8bd18b07581d8b652f1774bf7fee.webp"
width="760"
height="422"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="interactive-qa-session">Interactive Q&amp;amp;A Session:&lt;/h2>
&lt;p>The talks were followed by a vibrant interactive Q&amp;amp;A session, allowing attendees to engage directly with the speakers, fostering a rich exchange of ideas and clarifying complex topics in real-time.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="110301" srcset="
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_952a8bd18b07581d8b652f1774bf7fee.webp 400w,
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_a4dd9cd246dd4f9a1e10111d82773838.webp 760w,
/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://nlp-sig.github.io/media/110301_hu2606185e04be2ec2025eb2759171dac5_94746_952a8bd18b07581d8b652f1774bf7fee.webp"
width="760"
height="422"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>Socratic Question Generation: A Novel Dataset, Models, and Evaluation</title><link>https://nlp-sig.github.io/publication/socratic-question-generation-a-novel-dataset-models-and-evaluation/</link><pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/socratic-question-generation-a-novel-dataset-models-and-evaluation/</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2310.02905" target="_blank" rel="noopener">https://arxiv.org/abs/2310.02905&lt;/a>&lt;/p></description></item><item><title>From Deep Data Integration to Using LLMs to Query Unstructured and Structured Data</title><link>https://nlp-sig.github.io/event/from_deep_data_integration_to_using_llms_to_query_unstructured_and_structured_data/</link><pubDate>Tue, 26 Sep 2023 10:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/event/from_deep_data_integration_to_using_llms_to_query_unstructured_and_structured_data/</guid><description>&lt;p>We are witnessing the widespread adoption of deep learning techniques as avant-garde solutions to different computational problems in recent years. In data integration, the use of deep learning techniques has helped establish several state-of-the-art results in long standing problems, including information extraction, entity matching, data cleaning, and table understanding. In this talk, I will reflect on the strengths of deep learning and how that has helped move forward the needle in data integration. I will also discuss a few challenges associated with solutions based on deep learning techniques and describe some opportunities for future work.
Recently, Large Language Models (LLMs) have emerged as a powerful tool for accessing parametric knowledge, but the potential of tapping into the vast expanse of external or private data remains largely unexplored. This talk presents an open-source question-answering system for seamlessly integrating model parameters with knowledge from external data sources to enhance its predictive capabilities. Our larger vision transcends question answering. We envision a personal insight assistant, adept at sifting through your past data to offer invaluable insights to help make informed decisions and plan with foresight.&lt;/p>
&lt;p>Wang-Chiew is a research scientist at Meta AI. Before she was the Head of Research at Megagon Labs, where she led the research efforts on building advanced technologies to enhance search by experience. Prior to joining Megagon Labs, she was a Professor of Computer Science at the University of California, Santa Cruz. She also spent two years at IBM Research - Almaden. She received her B.Sc. (First Class) in Computer Science from the National University of Singapore and her Ph.D. in Computer Science from the University of Pennsylvania. Her research interests include data integration and exchange, data provenance, and natural language processing. She is the recipient of an NSF CAREER award, a Google Faculty Award, and an IBM Faculty Award. She co-authored best papers, she is a co-recipient of the 2014 ACM PODS Alberto O. Mendelzon Test-of-Time Award, the 2018 ICDT Test-of-Time Award, and the 2020 Alonzo Church Award. She received the 2019 VLDB Women in Database Research Award. She was on the VLDB Board of Trustees (2014-2019) and she is a Fellow of the ACM.&lt;/p></description></item><item><title>NLP SIG Kickoff Event</title><link>https://nlp-sig.github.io/event/kickoff/</link><pubDate>Wed, 23 Aug 2023 11:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/event/kickoff/</guid><description>&lt;p>Institute of Data Science played host to the much-anticipated Natural Language Processing (NLP) Special Interest Group (SIG) Kickoff Event on August 8, 2023.&lt;/p>
&lt;p>The Kickoff Event was a golden opportunity for NLP enthusiasts to gather and explore the latest trends in NLP research, connect, and spark collaborations. The NLP SIG at IDS was initiated with the vision of bringing together a community interested in the intersections of language with machine learning, AI, and data science. This Kickoff Event marked the formal commencement of this groundbreaking initiative.&lt;/p>
&lt;p>Stay tuned for more updates on future events and networking opportunities from NLP SIG at IDS. Join the league in shaping the future of natural language processing!&lt;/p></description></item><item><title>NLP SIG Kickoff Event</title><link>https://nlp-sig.github.io/post/sig-kickoff/</link><pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/post/sig-kickoff/</guid><description>&lt;p>Institute of Data Science played host to the much-anticipated Natural Language Processing (NLP) Special Interest Group (SIG) Kickoff Event on August 8, 2023.&lt;/p>
&lt;p>The Kickoff Event was a golden opportunity for NLP enthusiasts to gather and explore the latest trends in NLP research, connect, and spark collaborations.&lt;/p>
&lt;p>The NLP SIG at IDS was initiated with the vision of bringing together a community interested in the intersections of language with machine learning, AI, and data science. This Kickoff Event marked the formal commencement of this groundbreaking initiative.&lt;/p>
&lt;p>Stay tuned for more updates on future events and networking opportunities from NLP SIG at IDS. Join the league in shaping the future of natural language processing!&lt;/p></description></item><item><title>Rethinking the Visual Cues in Audio-Visual Speaker Extraction.</title><link>https://nlp-sig.github.io/publication/rethinking-the-visual-cues-in-audio-visual-speaker-extraction./</link><pubDate>Mon, 05 Jun 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/rethinking-the-visual-cues-in-audio-visual-speaker-extraction./</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2306.02625" target="_blank" rel="noopener">https://arxiv.org/abs/2306.02625&lt;/a>&lt;/p></description></item><item><title>DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding</title><link>https://nlp-sig.github.io/publication/demaformer-damped-exponential-moving-average-transformer-with-energy-based-modeling-for-temporal-language-grounding./</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/demaformer-damped-exponential-moving-average-transformer-with-energy-based-modeling-for-temporal-language-grounding./</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2306.09821" target="_blank" rel="noopener">https://arxiv.org/abs/2306.09821&lt;/a>&lt;/p></description></item><item><title>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System</title><link>https://nlp-sig.github.io/publication/unlocking-the-potential-of-user-feedbackleveraging-large-language-model-as-user-simulators-to-enhance-dialogue-system/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/unlocking-the-potential-of-user-feedbackleveraging-large-language-model-as-user-simulators-to-enhance-dialogue-system/</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2306.09821" target="_blank" rel="noopener">https://arxiv.org/abs/2306.09821&lt;/a>&lt;/p></description></item><item><title>Using Punctuation as an Adversarial Attack on Deep Learning-Based NLP Systems:An Empirical Study</title><link>https://nlp-sig.github.io/publication/using-punctuation-as-an-adversarial-attack-on-deep-learning-based-nlp-systems-an-empirical-study/</link><pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/using-punctuation-as-an-adversarial-attack-on-deep-learning-based-nlp-systems-an-empirical-study/</guid><description>&lt;p>Link: &lt;a href="https://aclanthology.org/2023.findings-eacl.1/" target="_blank" rel="noopener">https://aclanthology.org/2023.findings-eacl.1/&lt;/a>&lt;/p></description></item><item><title>Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process</title><link>https://nlp-sig.github.io/publication/modeling-user-satisfaction-dynamics-in-dialogue-via-hawkes-process/</link><pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/modeling-user-satisfaction-dynamics-in-dialogue-via-hawkes-process/</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2305.12594" target="_blank" rel="noopener">https://arxiv.org/abs/2305.12594&lt;/a>&lt;/p></description></item><item><title>Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations</title><link>https://nlp-sig.github.io/publication/knowledge-enhanced-mixed-initiative-dialogue-system-for-emotional-support-conversations/</link><pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/knowledge-enhanced-mixed-initiative-dialogue-system-for-emotional-support-conversations/</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2305.10172" target="_blank" rel="noopener">https://arxiv.org/abs/2305.10172&lt;/a>&lt;/p></description></item><item><title>Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction</title><link>https://nlp-sig.github.io/publication/gradient-boosted-decision-tree-for-listwise-context-model-in-multimodal-review-helpfulness-prediction/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/gradient-boosted-decision-tree-for-listwise-context-model-in-multimodal-review-helpfulness-prediction/</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2305.12678" target="_blank" rel="noopener">https://arxiv.org/abs/2305.12678&lt;/a>&lt;/p></description></item><item><title>PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network</title><link>https://nlp-sig.github.io/publication/piave-a-pose-invariant-audio-visual-speaker-extraction-network/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/piave-a-pose-invariant-audio-visual-speaker-extraction-network/</guid><description>&lt;p>Link: &lt;a href="https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html" target="_blank" rel="noopener">https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html&lt;/a>&lt;/p></description></item><item><title>Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration</title><link>https://nlp-sig.github.io/publication/prompting-and-evaluating-large-language-models-for-proactive-dialogues-clarification-target-guided-and-non-collaboration./</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/prompting-and-evaluating-large-language-models-for-proactive-dialogues-clarification-target-guided-and-non-collaboration./</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2305.13626" target="_blank" rel="noopener">https://arxiv.org/abs/2305.13626&lt;/a>&lt;/p></description></item><item><title>Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers</title><link>https://nlp-sig.github.io/publication/use-your-instinct-instruction-optimization-using-neural-bandits-coupled-with-transformers./</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/use-your-instinct-instruction-optimization-using-neural-bandits-coupled-with-transformers./</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2305.12678" target="_blank" rel="noopener">https://arxiv.org/abs/2305.12678&lt;/a>&lt;/p></description></item><item><title>Product Question Answering in E-Commerce: A Survey</title><link>https://nlp-sig.github.io/publication/product-question-answering-in-e-commerce-a-survey/</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/publication/product-question-answering-in-e-commerce-a-survey/</guid><description>&lt;p>Link: &lt;a href="https://arxiv.org/abs/2302.08092" target="_blank" rel="noopener">https://arxiv.org/abs/2302.08092&lt;/a>&lt;/p></description></item><item><title>Contact</title><link>https://nlp-sig.github.io/contact/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/contact/</guid><description/></item><item><title>People</title><link>https://nlp-sig.github.io/people/</link><pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/people/</guid><description/></item><item><title/><link>https://nlp-sig.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nlp-sig.github.io/admin/config.yml</guid><description/></item></channel></rss>